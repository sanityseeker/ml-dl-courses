{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод главных компонент для уменьшения размерности\n",
    "\n",
    "В файле `airbnb-100k.tsv` лежит набор данных об апартаментах на сайте AirBnb. Для наших целей взята урезанная версия. Полную версию набора данных можно скачать <a href=\"https://data.opendatasoft.com/explore/dataset/airbnb-ratings%40public/table/?disjunctive.city&disjunctive.neighbourhood_cleansed&sort=number_of_reviews\"> по ссылке </a>.\n",
    "\n",
    "Поставим перед собой следующую задачу - по текстовому описанию апартаментов угадать цену за апартаменты. Если бы это была реальная задача, то мы использовали все доступные данные, однако для текущих целей нам будет достаточно исключительно текстового описания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:23:09.638809Z",
     "start_time": "2022-02-10T23:23:09.492916Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:23:10.614906Z",
     "start_time": "2022-02-10T23:23:10.171663Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"airbnb-100k.tsv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:23:11.707880Z",
     "start_time": "2022-02-10T23:23:11.700066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>Hi everyone, Cosy bedroom in a modern apartmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.0</td>\n",
       "      <td>Very comfortable and calm apartment, 2 rooms i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59.0</td>\n",
       "      <td>At a few minutes by walking from République, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>Come stay here in my little nest. It's a very ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.0</td>\n",
       "      <td>Studio de 25 m2, idéalement situé au centre de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Price                                        Description\n",
       "0   50.0  Hi everyone, Cosy bedroom in a modern apartmen...\n",
       "1  125.0  Very comfortable and calm apartment, 2 rooms i...\n",
       "2   59.0  At a few minutes by walking from République, O...\n",
       "3   50.0  Come stay here in my little nest. It's a very ...\n",
       "4   48.0  Studio de 25 m2, idéalement situé au centre de..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:23:12.400010Z",
     "start_time": "2022-02-10T23:23:12.395990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Price', 'Description'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:23:13.497115Z",
     "start_time": "2022-02-10T23:23:13.475904Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['Price', \"Description\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:23:13.923397Z",
     "start_time": "2022-02-10T23:23:13.920501Z"
    }
   },
   "outputs": [],
   "source": [
    "Y = df['Price']\n",
    "X = df['Description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока набор данных выглядит достаточно скромно, однако ситуация изменится, когда мы подготовим их к обучению. Воспользуемся методом Bag-of-words для кодирования каждого описания. То есть под каждое слово выделим отдельный бинарный признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:23:15.298785Z",
     "start_time": "2022-02-10T23:23:15.166305Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:23:15.854297Z",
     "start_time": "2022-02-10T23:23:15.850951Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()  # Преобразователь для слов в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:23:20.631068Z",
     "start_time": "2022-02-10T23:23:16.362898Z"
    }
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(X)  # Считаем признаки и сразу трансформируем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:23:22.294049Z",
     "start_time": "2022-02-10T23:23:22.240523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127133"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вышло больше 100 000 признаков уже на таком небольшом наборе данных. Итого на текущий момент мы уже оперируем матрицей 100000 х 127133."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:23:24.591799Z",
     "start_time": "2022-02-10T23:23:24.534356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boeuf',\n",
       " 'bofa',\n",
       " 'boffi',\n",
       " 'bofinger',\n",
       " 'bog',\n",
       " 'bogart',\n",
       " 'bogatel',\n",
       " 'bogatell',\n",
       " 'bogedas',\n",
       " 'boggling']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[20000:20010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучиться напрямую на полученных признаках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:23:36.160678Z",
     "start_time": "2022-02-10T23:23:36.139394Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:23:38.082604Z",
     "start_time": "2022-02-10T23:23:38.044102Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве модели возьмем самую обычную линейную регрессию с L2 регуляризацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:24:00.115851Z",
     "start_time": "2022-02-10T23:23:40.143429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 36s, sys: 1.28 s, total: 2min 38s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "regressor = Ridge(solver='sparse_cg').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подсчета алгоритму потребовалось ~ 60 секунд\n",
    "\n",
    "Давайте подсчитаем метрику качества R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:24:06.202175Z",
     "start_time": "2022-02-10T23:24:06.192712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2045650378712981\n"
     ]
    }
   ],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На текущих данных вышло примерно 0.20 , что в абсолютных значениях конечно очень мало\n",
    "\n",
    "Однако нужно учитывать, что мы используем только описание и никаким дополнительным образом данные не обрабатывали. Так что будем смотреть на это значение только как на опорное, относительно которого мы будем изменять наши дальнейшие модели.\n",
    "\n",
    "Дальнейший наш шаг - это сжатие этой гигантской матрицы - воспользуемся методом главных компонент, а точнее методом `TruncatedSVD`, который умеет эффективно работать с разреженными данными и считать только необходимое количество компонент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:24:08.001961Z",
     "start_time": "2022-02-10T23:24:07.992428Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T23:24:48.680273Z",
     "start_time": "2022-02-10T23:24:48.678483Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = TruncatedSVD(n_components=100, random_state=0)  # Считаем 100 главных компонент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-10T23:24:58.336Z"
    }
   },
   "outputs": [],
   "source": [
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчет 100 компонент занял примерно 25 секунд\n",
    "\n",
    "Подсчитаем теперь новые признаки и отправим в обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.60080609e+00,  1.20545928e-01, -3.12077559e-01, -9.35900168e-01,\n",
       "        1.06623032e+00, -3.42617726e-01,  3.40108026e-01, -1.49998690e+00,\n",
       "       -6.90031525e-01, -5.50001400e-01, -2.05049310e+00,  7.10911183e-01,\n",
       "       -9.71387064e-01, -6.85784036e-01, -4.18490309e-01,  5.96567946e-01,\n",
       "        1.04840018e+00, -2.75965332e-01,  3.21515558e-01, -8.12669877e-03,\n",
       "        5.34935786e-02,  3.16888913e-01, -2.92356506e-01, -1.69813752e-01,\n",
       "        8.74725171e-01, -6.23044587e-01,  2.86307943e-01,  7.89611385e-01,\n",
       "        1.77055459e-01, -2.60638395e-01,  1.98627867e-01,  3.18454224e-01,\n",
       "       -4.57584471e-01, -5.46850163e-01,  1.11635487e+00,  4.75002002e-01,\n",
       "        1.51872696e-01,  1.71561238e-01,  2.63896805e-01, -9.72763919e-01,\n",
       "       -8.73070723e-02,  3.12098563e-01,  5.77297661e-01, -1.51388305e-01,\n",
       "       -5.14977488e-02,  2.97400807e-02, -1.23483435e-01,  2.84605426e-01,\n",
       "       -2.78054775e-01, -3.61637246e-01, -8.86492910e-01, -4.33508324e-01,\n",
       "        7.42200019e-01,  3.85924102e-02,  9.68162341e-02, -3.35776907e-01,\n",
       "       -1.08474372e-01, -1.26027526e+00, -4.47622218e-02,  1.74937811e-01,\n",
       "       -6.47139347e-02,  1.81117712e-01, -2.56990087e-02, -1.77267438e-01,\n",
       "        3.34769373e-03, -3.53602354e-01,  3.96362827e-02, -2.04280134e-01,\n",
       "        4.01189238e-01, -1.49981610e-01, -7.43379721e-01, -3.11298328e-01,\n",
       "        1.91688131e-01,  1.07196140e-01, -1.28436339e-02,  3.44170251e-02,\n",
       "        4.02996069e-01,  8.83498052e-01, -2.34282273e-01,  1.55829006e-01,\n",
       "        1.60262533e-01,  1.43346146e-01, -5.12903596e-02,  6.27416672e-01,\n",
       "       -5.58484724e-01, -2.65841206e-01, -1.89945131e-01, -4.41605790e-01,\n",
       "       -1.49710339e-01, -1.04640359e-01,  2.52406807e-01, -5.09444821e-01,\n",
       "       -1.21561515e-01, -3.64517698e-01, -1.12927714e-01,  3.57538736e-02,\n",
       "        2.75454100e-01,  5.65759011e-01,  6.01427256e-01, -1.85196163e-01])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new[0]  # Как теперь выглядят признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, Y, test_size=0.33, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.21 s, sys: 1.74 s, total: 2.95 s\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "regressor = Ridge(solver='sparse_cg').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почти моментально! На моей машине вышло меньше секунды для обучения\n",
    "\n",
    "Посмотрим, что с качеством"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13868347074457865"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_pred = regressor.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что качество немного просело относительно изначальной модели, однако ресурсов для вычисления было задействовано значительно меньше.\n",
    "\n",
    "Размер данных изменился с 100000x127151 до 100000x100.\n",
    "При этом времени потребовалось в **2.5 раза** меньше - 22 секунды суммарно у PCA + Ridge против 60 секунд у raw Ridge\n",
    "\n",
    "Однако за это мы заплатили немного качеством модели. Давайте попробуем выжать больше из данных и подсчитаем 200 компонент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 1min 17s, total: 2min 38s\n",
      "Wall time: 1min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=200, random_state=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pca = TruncatedSVD(n_components=200, random_state=0)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, Y, test_size=0.33, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.97 s, sys: 2.87 s, total: 6.84 s\n",
      "Wall time: 3.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "regressor = Ridge(solver='sparse_cg').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2089416680655557"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_pred = regressor.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество даже немного выросло по сравнению с линейной регрессией на сырых признаках! Это скорее всего связано с тем, что оставшаяся информация в данных, которую мы выкинули, являлась шумом и больше мешала модели что-то понять, чем наоборот.\n",
    "\n",
    "При этом по времени мы все еще сильно выигрываем по времени примерно в **1.5 раза** - 37 секунд против 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ради интереса можем посмотреть, как выглядят главные компоненты с точки зрения слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.593 x the\n",
      "0.468 x and\n",
      "0.267 x to\n",
      "0.237 x is\n",
      "0.22 x in\n",
      "0.203 x of\n",
      "0.183 x with\n",
      "0.152 x you\n",
      "0.104 x for\n",
      "0.0866 x room\n"
     ]
    }
   ],
   "source": [
    "component = pca.components_[0].argsort()[-10:][::-1]\n",
    "\n",
    "for c, word in zip([pca.components_[0][v] for v in component], [vectorizer.get_feature_names()[v] for v in component]):\n",
    "    print(\"{:.3} x {}\".format(c, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0108 x de\n",
      "0.0073 x la\n",
      "0.00195 x et\n",
      "0.00153 x un\n",
      "0.00364 x en\n",
      "0.002 x le\n",
      "0.000892 x est\n",
      "0.00118 x con\n",
      "0.00056 x une\n",
      "0.00186 x du\n"
     ]
    }
   ],
   "source": [
    "component = pca.components_[1].argsort()[-10:][::-1]\n",
    "\n",
    "for c, word in zip([pca.components_[0][v] for v in component], [vectorizer.get_feature_names()[v] for v in component]):\n",
    "    print(\"{:.3} x {}\".format(c, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0498 x we\n",
      "0.0359 x our\n",
      "0.0367 x house\n",
      "0.0665 x are\n",
      "0.0866 x room\n",
      "0.0309 x home\n",
      "0.0516 x have\n",
      "0.104 x for\n",
      "0.0306 x private\n",
      "0.22 x in\n"
     ]
    }
   ],
   "source": [
    "component = pca.components_[10].argsort()[-10:][::-1]\n",
    "\n",
    "for c, word in zip([pca.components_[0][v] for v in component], [vectorizer.get_feature_names()[v] for v in component]):\n",
    "    print(\"{:.3} x {}\".format(c, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Расчеты на кластере\n",
    "\n",
    "В библиотеке Spark присутствуют уже готовые инструменты для подсчета главных компонент. Таким образом если наши данные не вмещаются на одну машину, у нас есть шанс воспользоваться этим методом, хоть это может оказаться и более ресурсоемко."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName(\"CourseraLocalSpark\").setMaster(\"local[*]\")\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.appName('PCA-examples').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb = spark.read.option(\"header\",True).option(\"parserLib\", \"univocity\")\\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .csv('airbnb-100k.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airbnb.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb.createOrReplaceTempView(\"airbnb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После загрузки достаем данные только по Description и сразу разбиваем строку на отдельные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.sql(\"\"\"\n",
    "    SELECT split(Description, ' ') as Words\n",
    "    FROM airbnb\n",
    "    WHERE Description is not Null\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вновь собираем Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"Words\", outputCol=\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_vectorizer = cv.fit(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spark_vectorizer.transform(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               Words|            Features|\n",
      "+--------------------+--------------------+\n",
      "|[Hi, everyone,, C...|(262144,[0,1,2,3,...|\n",
      "|[\"Very, comfortab...|(262144,[0,1,2,3,...|\n",
      "|[At, a, few, minu...|(262144,[0,1,2,3,...|\n",
      "|[\"Come, stay, her...|(262144,[0,1,2,3,...|\n",
      "|[\"Studio, de, 25,...|(262144,[0,1,2,3,...|\n",
      "|[\"3, rooms, appar...|(262144,[0,1,2,3,...|\n",
      "|[Cozy, &, quiet, ...|(262144,[0,1,2,3,...|\n",
      "|[Je, vous, accuei...|(262144,[14,53,88...|\n",
      "|[My, apartment, i...|(262144,[0,1,2,3,...|\n",
      "|[Studio, de, cara...|(262144,[7,14,33,...|\n",
      "|[Situated, in, a,...|(262144,[0,1,2,3,...|\n",
      "|[\"Cozy, studio, i...|(262144,[0,1,2,3,...|\n",
      "|[Small, room, wit...|(262144,[8,17,129...|\n",
      "|[Un, appartement,...|(262144,[7,14,23,...|\n",
      "|[2, pièces, dans,...|(262144,[23,146,1...|\n",
      "|[Chambre, privati...|(262144,[14,272,4...|\n",
      "|[In, the, 11th, d...|(262144,[1,2,5,6,...|\n",
      "|[Charmant, et, co...|(262144,[2,7,14,4...|\n",
      "|[Agréable, chambr...|(262144,[2,7,14,2...|\n",
      "|[Cozy, apartment,...|(262144,[0,1,2,3,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее нам потребуется инструментарий для линейной алгебры, чтобы решить задачу PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В самом библиотеке есть несколько тонких моментов - авторы поддерживают предыдущие версии библиотеки, поэтому иногда требуется в ручном режиме конвертировать объекты из старых форматов в новые. Так для того, чтобы PCA отработал, нам необходимо сконвертировать формат векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_raw = MLUtils.convertVectorColumnsFromML(X, \"Features\").select('Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Features|\n",
      "+--------------------+\n",
      "|(262144,[0,1,2,3,...|\n",
      "|(262144,[0,1,2,3,...|\n",
      "|(262144,[0,1,2,3,...|\n",
      "|(262144,[0,1,2,3,...|\n",
      "|(262144,[0,1,2,3,...|\n",
      "|(262144,[0,1,2,3,...|\n",
      "|(262144,[0,1,2,3,...|\n",
      "|(262144,[14,53,88...|\n",
      "|(262144,[0,1,2,3,...|\n",
      "|(262144,[7,14,33,...|\n",
      "|(262144,[0,1,2,3,...|\n",
      "|(262144,[0,1,2,3,...|\n",
      "|(262144,[8,17,129...|\n",
      "|(262144,[7,14,23,...|\n",
      "|(262144,[23,146,1...|\n",
      "|(262144,[14,272,4...|\n",
      "|(262144,[1,2,5,6,...|\n",
      "|(262144,[2,7,14,4...|\n",
      "|(262144,[2,7,14,2...|\n",
      "|(262144,[0,1,2,3,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = RowMatrix(features_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.98 s, sys: 0 ns, total: 1.98 s\n",
      "Wall time: 4min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pca = matrix.computePrincipalComponents(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь можно отметить, что времени на расчет потребовалось больше, чем когда мы использовали sklearn. Это связано с двумя вещами\n",
    "* Сама реализация алгоритма в библиотеке Spark отличается\n",
    "* В нем произведены оптимизации именно под большие данные, которые можно считать на кластере из множества машин. Однако сейчас мы запускаем с вами Spark на одной машине на очень небольшом размере данных, которые используем для примера. Таким образом все эти оптимизации добавляют лишь лишние вычисления, тем самым замедляя работу на нашем примере. Если запускать алгоритм на настоящем кластере на действительно больших данных, то результат должен быть лучше sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(262144, 10, [0.4776, 0.4742, 0.3545, 0.2703, 0.2405, 0.2073, 0.2052, 0.1804, ..., 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, -0.0, 0.0], 0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = matrix.multiply(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_rdd = new_features.rows.map(lambda x: Row(Features=x.asML()))\n",
    "PCA_X = spark.createDataFrame(PCA_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Features|\n",
      "+--------------------+\n",
      "|[5.32238352554188...|\n",
      "|[12.1447150453716...|\n",
      "|[13.5061343026504...|\n",
      "|[16.2374323088063...|\n",
      "|[4.58042146968792...|\n",
      "|[15.7023499390735...|\n",
      "|[13.0295803721516...|\n",
      "|[0.04802258073030...|\n",
      "|[19.4428614831905...|\n",
      "|[0.51447737098622...|\n",
      "|[11.8417923523528...|\n",
      "|[4.21237840711190...|\n",
      "|[0.26304743615421...|\n",
      "|[0.30656949374429...|\n",
      "|[0.04550437667744...|\n",
      "|[0.04748234001660...|\n",
      "|[2.02008498469833...|\n",
      "|[1.15976206148425...|\n",
      "|[1.04475349498805...|\n",
      "|[8.47642640188163...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PCA_X.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы с вами подсчитали новые признаки методом главных компонент на кластере."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Важно отметить, что PCA - универсальный алгоритм сжатия, который умеет работать с любым набором данных, который закодирован в виде матрицы. С помощью него можно сжимать не только текста или разреженные данные, а вообще все что угодно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Альтернативные сжиматели\n",
    "\n",
    "Помимо PCA можем использовать и более продвинутые способы сжатия размерности, однако из-за их вычислительной сложности их использование на больших данных может быть нецелесообразным.\n",
    "\n",
    "Попробуем например сжать через t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Description']\n",
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 35s, sys: 0 ns, total: 4min 35s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X[:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 15.161038 , -13.1261635],\n",
       "       [  8.769115 ,   7.392556 ],\n",
       "       [-12.876113 ,  11.121072 ],\n",
       "       ...,\n",
       "       [ 18.962418 ,  14.771621 ],\n",
       "       [ 16.30225  ,  -4.4295483],\n",
       "       [  8.517254 ,  -8.509135 ]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На подсчет 2 компонент для всего 20000 ушло почти 2 минуты. \n",
    "\n",
    "Таким образом PCA может оказаться единственным способом сжатия размерности, если речь зайдет про значительные объемы данных. \n",
    "\n",
    "Однако стоит отметить, что некоторые типы данных могут иметь свои спецефичные способы сжатия - например тексты или картинки - которые могут быть запущены на больших объемах данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
