{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j81mzLx40eD"
   },
   "source": [
    "## PyTorch. DistributedDataParallel \n",
    "\n",
    "Это класс, скрывающий под капотом детали параллельного обучениия в PyTorch, применяется для обучения на кластере из нескольких компьютеров с несколькими GPU (поэтому используйте google colab для запуска этого ноутбука - https://colab.research.google.com/)\n",
    "\n",
    "Внутри осуществляет разбивку данных по обработчикам, на backward шаге градиенты усредняются с использованием allreduce.\n",
    "\n",
    "Существует также `DataParallel` класс, но он работает в рамках одного процесса, используя потоки, тем самым он не рекомендуется к применению в силу ограничений GIL, `DistributedDataParallel` показывает себя лучше даже при обучении на одном компьютере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9i8Z9Ku4wGU",
    "outputId": "ac59f105-ba82-4346-e57f-32d98385b60f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting launch_ddp_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile launch_ddp_demo.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # инициализация группы процессов\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "\n",
    "\n",
    "def demo_basic(rank, world_size):\n",
    "    print(f\"Базовый пример использованиия DDP с рангом {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # создать модель и отправить её на GPU с id = rank\n",
    "    model = ToyModel()\n",
    "    ddp_model = DDP(model)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "def run_demo(demo_fn, world_size):\n",
    "    mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True)\n",
    "    \n",
    "def demo_checkpoint(rank, world_size):\n",
    "    print(f\"Пример использованиия DDP с чекпоинтами с рангом {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n",
    "    if rank == 0:\n",
    "        # Все процессы должны начать с одних значений параметров и градиента,\n",
    "        # они также синхронизируются на backward шаге,\n",
    "        # поэтому достаточно сохранять модель в одном процессе\n",
    "        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n",
    "\n",
    "    # Используем barrier(), чтобы удостовериться,\n",
    "    # что процесс 1 загрузит модель после сохранения процессом 0\n",
    "    dist.barrier()\n",
    "    # опциии map_location\n",
    "    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n",
    "    ddp_model.load_state_dict(\n",
    "        torch.load(CHECKPOINT_PATH, map_location=map_location))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if rank == 0:\n",
    "        os.remove(CHECKPOINT_PATH)\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "class ToyMpModel(nn.Module):\n",
    "    def __init__(self, dev0, dev1):\n",
    "        super(ToyMpModel, self).__init__()\n",
    "        self.dev0 = dev0\n",
    "        self.dev1 = dev1\n",
    "        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.dev0)\n",
    "        x = self.relu(self.net1(x))\n",
    "        x = x.to(self.dev1)\n",
    "        return self.net2(x)\n",
    "\n",
    "def demo_model_parallel(rank, world_size):\n",
    "    print(f\"Пример использованиия DDP с параллельностью по модели с рангом {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # настроим модель и устройства в этом процессе\n",
    "    dev0 = rank * 2\n",
    "    dev1 = rank * 2 + 1\n",
    "    mp_model = ToyMpModel(dev0, dev1)\n",
    "    ddp_mp_model = DDP(mp_model)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # вывод будет на устройстве dev1\n",
    "    outputs = ddp_mp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(dev1)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(\"Количество GPU:\", n_gpus)\n",
    "    run_demo(demo_basic, 4)\n",
    "    run_demo(demo_checkpoint, n_gpus)\n",
    "    # run_demo(demo_model_parallel, n_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pTnDeykLA8wJ",
    "outputId": "62866f29-7c09-428d-f640-972c0bd6a291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество GPU: 0\n",
      "Базовый пример использованиия DDP с рангом 2.\n",
      "Базовый пример использованиия DDP с рангом 1.\n",
      "Базовый пример использованиия DDP с рангом 0.\n",
      "Базовый пример использованиия DDP с рангом 3.\n"
     ]
    }
   ],
   "source": [
    "! python3 launch_ddp_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0YIfRC049CX"
   },
   "source": [
    "Для сохранениия и загрузки состояния обучения (checkpoint) используются функциии `torch.save()` и `torch.load()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJispqHd5C88"
   },
   "source": [
    "Model Parallel - использование нескольких устройств на одном обработчике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_-HisF2HzVg"
   },
   "source": [
    "### Обучение MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oj92iYqL9wal",
    "outputId": "b3c1e4ef-bc34-444d-8783-f243e7deabb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing launch_ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile launch_ddp.py\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N',\n",
    "                        help='количество обработчиков (default: 1)')\n",
    "    parser.add_argument('-g', '--gpus', default=1, type=int,\n",
    "                        help='число GPU на каждом обработчике')\n",
    "    parser.add_argument('-nr', '--nr', default=0, type=int,\n",
    "                        help='глобальный ранг')\n",
    "    parser.add_argument('--epochs', default=2, type=int, metavar='N',\n",
    "                        help='количество эпох обучениия')\n",
    "    args = parser.parse_args()\n",
    "    args.world_size = args.gpus * args.nodes\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '23333'\n",
    "    mp.spawn(train, nprocs=args.gpus, args=(args,))\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train(gpu, args):\n",
    "    rank = args.nr * args.gpus + gpu\n",
    "    dist.init_process_group(backend='nccl', init_method='env://', world_size=args.world_size, rank=rank)\n",
    "    torch.manual_seed(0)\n",
    "    model = ConvNet()\n",
    "    torch.cuda.set_device(gpu)\n",
    "    model.cuda(gpu)\n",
    "    batch_size = 100\n",
    "    # определим loss function и optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 1e-4)\n",
    "    # Обернем модель в DDP\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[gpu])\n",
    "    # Загрузка данных\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                               train=True,\n",
    "                                               transform=transforms.ToTensor(),\n",
    "                                               download=True)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,\n",
    "                                                                    num_replicas=args.world_size,\n",
    "                                                                    rank=rank)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True,\n",
    "                                               sampler=train_sampler)\n",
    "\n",
    "    start = datetime.now()\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(args.epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i + 1) % 100 == 0 and gpu == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, args.epochs, i + 1, total_step,\n",
    "                                                                         loss.item()))\n",
    "    if gpu == 0:\n",
    "        print(\"Обучение завершено за: \" + str(datetime.now() - start))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8WfZ2cl590da",
    "outputId": "3871f1b6-9236-45ff-b99a-330e4a9e75e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W CUDAFunctions.cpp:100] Warning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (function operator())\n",
      "Traceback (most recent call last):\n",
      "  File \"launch_ddp.py\", line 104, in <module>\n",
      "    main()\n",
      "  File \"launch_ddp.py\", line 28, in main\n",
      "    mp.spawn(train, nprocs=args.gpus, args=(args,))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 199, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 157, in start_processes\n",
      "    while not context.join():\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\n",
      "    raise Exception(msg)\n",
      "Exception: \n",
      "\n",
      "-- Process 0 terminated with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\n",
      "    fn(i, *args)\n",
      "  File \"/home/jovyan/work/launch_ddp.py\", line 56, in train\n",
      "    dist.init_process_group(backend='nccl', init_method='env://', world_size=args.world_size, rank=rank)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 433, in init_process_group\n",
      "    timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 519, in _new_process_group_helper\n",
      "    timeout)\n",
      "RuntimeError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python3 launch_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYuKlRN4KEu5"
   },
   "source": [
    "### Parameter Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUL7W_SP5GgI",
    "outputId": "6a81de25-d3b5-4094-be21-bf0f291be97d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing launcher.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile launcher.py\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from threading import Lock\n",
    "\n",
    "import torch\n",
    "import torch.distributed.autograd as dist_autograd\n",
    "import torch.distributed.rpc as rpc\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.distributed.optim import DistributedOptimizer\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# --------- MNIST Network to train, from pytorch/examples -----\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_gpus=0):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_gpus = num_gpus\n",
    "        device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() and self.num_gpus > 0 else \"cpu\")\n",
    "        print(f\"Putting first 2 convs on {str(device)}\")\n",
    "        # Разместим conv слои на устройстве\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1).to(device)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1).to(device)\n",
    "        # Остальное - на другие устройства, если онии есть\n",
    "        if \"cuda\" in str(device) and num_gpus > 1:\n",
    "            device = torch.device(\"cuda:1\")\n",
    "\n",
    "        print(f\"Putting rest of layers on {str(device)}\")\n",
    "        self.dropout1 = nn.Dropout2d(0.25).to(device)\n",
    "        self.dropout2 = nn.Dropout2d(0.5).to(device)\n",
    "        self.fc1 = nn.Linear(9216, 128).to(device)\n",
    "        self.fc2 = nn.Linear(128, 10).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Переместим тензор на следующее устройство, если требуется\n",
    "        next_device = next(self.fc1.parameters()).device\n",
    "        x = x.to(next_device)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "# --------- Helper Methods --------------------\n",
    "def call_method(method, rref, *args, **kwargs):\n",
    "    return method(rref.local_value(), *args, **kwargs)\n",
    "\n",
    "def remote_method(method, rref, *args, **kwargs):\n",
    "    args = [method, rref] + list(args)\n",
    "    return rpc.rpc_sync(rref.owner(), call_method, args=args, kwargs=kwargs)\n",
    "\n",
    "\n",
    "# --------- Parameter Server --------------------\n",
    "class ParameterServer(nn.Module):\n",
    "    def __init__(self, num_gpus=0):\n",
    "        super().__init__()\n",
    "        model = Net(num_gpus=num_gpus)\n",
    "        self.model = model\n",
    "        self.input_device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() and num_gpus > 0 else \"cpu\")\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = inp.to(self.input_device)\n",
    "        out = self.model(inp)\n",
    "        out = out.to(\"cpu\")\n",
    "        return out\n",
    "\n",
    "    def get_dist_gradients(self, cid):\n",
    "        grads = dist_autograd.get_gradients(cid)\n",
    "        cpu_grads = {}\n",
    "        for k, v in grads.items():\n",
    "            k_cpu, v_cpu = k.to(\"cpu\"), v.to(\"cpu\")\n",
    "            cpu_grads[k_cpu] = v_cpu\n",
    "        return cpu_grads\n",
    "\n",
    "    def get_param_rrefs(self):\n",
    "        param_rrefs = [rpc.RRef(param) for param in self.model.parameters()]\n",
    "        return param_rrefs\n",
    "\n",
    "\n",
    "param_server = None\n",
    "global_lock = Lock()\n",
    "\n",
    "\n",
    "def get_parameter_server(num_gpus=0):\n",
    "    global param_server\n",
    "    with global_lock:\n",
    "        if not param_server:\n",
    "            # Создаём один раз\n",
    "            param_server = ParameterServer(num_gpus=num_gpus)\n",
    "        return param_server\n",
    "\n",
    "\n",
    "def run_parameter_server(rank, world_size):\n",
    "    print(\"PS master стартует RPC\")\n",
    "    rpc.init_rpc(name=\"parameter_server\", rank=rank, world_size=world_size)\n",
    "    print(\"RPC инициализация! Стартую параметрический сервер...\")\n",
    "    rpc.shutdown()\n",
    "    print(\"RPC выключение параметрического сервера.\")\n",
    "\n",
    "\n",
    "# --------- Trainers --------------------\n",
    "\n",
    "class TrainerNet(nn.Module):\n",
    "    def __init__(self, num_gpus=0):\n",
    "        super().__init__()\n",
    "        self.num_gpus = num_gpus\n",
    "        self.param_server_rref = rpc.remote(\n",
    "            \"parameter_server\", get_parameter_server, args=(num_gpus,))\n",
    "\n",
    "    def get_global_param_rrefs(self):\n",
    "        remote_params = remote_method(\n",
    "            ParameterServer.get_param_rrefs,\n",
    "            self.param_server_rref)\n",
    "        return remote_params\n",
    "\n",
    "    def forward(self, x):\n",
    "        model_output = remote_method(\n",
    "            ParameterServer.forward, self.param_server_rref, x)\n",
    "        return model_output\n",
    "\n",
    "\n",
    "def run_training_loop(rank, num_gpus, train_loader, test_loader):\n",
    "    # forward + backward + optimizer step, но распределенно.\n",
    "    net = TrainerNet(num_gpus=num_gpus)\n",
    "    # Создадим DistributedOptmizer.\n",
    "    param_rrefs = net.get_global_param_rrefs()\n",
    "    opt = DistributedOptimizer(optim.SGD, param_rrefs, lr=0.03)\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        with dist_autograd.context() as cid:\n",
    "            model_output = net(data)\n",
    "            target = target.to(model_output.device)\n",
    "            loss = F.nll_loss(model_output, target)\n",
    "            if i % 5 == 0:\n",
    "                print(f\"Rank {rank} training batch {i} loss {loss.item()}\")\n",
    "            dist_autograd.backward(cid, [loss])\n",
    "            # Проверяем, что ответ получен\n",
    "            assert remote_method(\n",
    "                ParameterServer.get_dist_gradients,\n",
    "                net.param_server_rref,\n",
    "                cid) != {}\n",
    "            opt.step(cid)\n",
    "\n",
    "    print(\"Обучение завершено!\")\n",
    "    print(\"Считаем accuracy....\")\n",
    "    get_accuracy(test_loader, net)\n",
    "\n",
    "\n",
    "def get_accuracy(test_loader, model):\n",
    "    model.eval()\n",
    "    correct_sum = 0\n",
    "    # Используем GPU для подсчёта если возможно\n",
    "    device = torch.device(\"cuda:0\" if model.num_gpus > 0 \n",
    "        and torch.cuda.is_available() else \"cpu\")\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1, keepdim=True)\n",
    "            pred, target = pred.to(device), target.to(device)\n",
    "            correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "            correct_sum += correct\n",
    "\n",
    "    print(f\"Accuracy {correct_sum / len(test_loader.dataset)}\")\n",
    "\n",
    "\n",
    "# Main loop для обучающих обработчиков\n",
    "def run_worker(rank, world_size, num_gpus, train_loader, test_loader):\n",
    "    print(f\"Worker rank {rank} initializing RPC\")\n",
    "    rpc.init_rpc(\n",
    "        name=f\"trainer_{rank}\",\n",
    "        rank=rank,\n",
    "        world_size=world_size)\n",
    "\n",
    "    print(f\"Worker {rank} done initializing RPC\")\n",
    "\n",
    "    run_training_loop(rank, num_gpus, train_loader, test_loader)\n",
    "    rpc.shutdown()\n",
    "\n",
    "# --------- Launcher --------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    world_size = 2\n",
    "    master_port = '29500'\n",
    "    num_gpus = 0\n",
    "    master_addr = 'localhost'\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Parameter-Server RPC обучение\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--rank\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Глобальный rank процесса. 0 - параметрический сервер.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    assert args.rank is not None, \"Следует указать ранг.\"\n",
    "    os.environ['MASTER_ADDR'] = master_addr\n",
    "    os.environ[\"MASTER_PORT\"] = master_port\n",
    "    processes = []\n",
    "    if args.rank == 0:\n",
    "        p = mp.Process(target=run_parameter_server, args=(0, world_size))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    else:\n",
    "        # Данные для обучения\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST('./data', train=True, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                            ])),\n",
    "            batch_size=32, shuffle=True,)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST(\n",
    "                './data',\n",
    "                train=False,\n",
    "                transform=transforms.Compose(\n",
    "                    [\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(\n",
    "                            (0.1307,\n",
    "                              ),\n",
    "                            (0.3081,\n",
    "                              ))])),\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        # стартуем обучение\n",
    "        p = mp.Process(\n",
    "            target=run_worker,\n",
    "            args=(\n",
    "                args.rank,\n",
    "                world_size,num_gpus,\n",
    "                train_loader,\n",
    "                test_loader))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xR5D162v6fr3",
    "outputId": "f63c3bd0-c6dd-4084-b5d7-52c5dec81479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing starter.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile starter.sh\n",
    "\n",
    "python3 launcher.py --rank 0 &\n",
    "sleep 5\n",
    "python3 launcher.py --rank 1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ilMJtLvw7mkd",
    "outputId": "5b907929-f3a1-466a-edf5-9b55e8be8861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PS master стартует RPC\r\n"
     ]
    }
   ],
   "source": [
    "! chmod +x ./starter.sh\n",
    "! ./starter.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5BPZ_e-7xOO",
    "outputId": "317becfc-0eaf-4203-a70c-c481a62ab307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jovyan        23  0.0  0.0 644812 40884 ?        Ssl  15:51   0:01 /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan/.local/share/jupyter/runtime/kernel-33a2fc5a-e1b8-4510-b02a-d602aa6de2c1.json\r\n",
      "jovyan       759  0.0  0.0   4632   644 pts/0    Ss+  16:42   0:00 /bin/sh -c  ps aux | grep launch\r\n",
      "jovyan       761  0.0  0.0  13140  1284 pts/0    S+   16:42   0:00 grep launch\r\n"
     ]
    }
   ],
   "source": [
    "! ps aux | grep launch"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "week-10-DDP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
