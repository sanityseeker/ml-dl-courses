{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j81mzLx40eD"
   },
   "source": [
    "### Распределенное обучениеи с использованием PyTorch и Horovod для MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используйте Google Colab для запуска этого ноутбука - https://colab.research.google.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9i8Z9Ku4wGU",
    "outputId": "b7b12847-4286-4701-d32d-e34936914401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting learn_hvd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile learn_hvd.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "# Параметры обучения\n",
    "batch_size = 100\n",
    "num_epochs = 5\n",
    "momentum = 0.5\n",
    "log_interval = 100\n",
    "\n",
    "def train_one_epoch(model, device, data_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader) * len(data),\n",
    "                100. * batch_idx / len(data_loader), loss.item()))\n",
    "            \n",
    "from time import time\n",
    "import os\n",
    "\n",
    "LOG_DIR = os.path.join('./logs/', str(time()), 'MNISTDemo')\n",
    "os.makedirs(LOG_DIR)\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch):\n",
    "  filepath = LOG_DIR + '/checkpoint-{epoch}.pth.tar'.format(epoch=epoch)\n",
    "  state = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "  }\n",
    "  torch.save(state, filepath)\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def train(learning_rate):\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  train_dataset = datasets.MNIST(\n",
    "    'data', \n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "  data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  model = Net().to(device)\n",
    "\n",
    "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    train_one_epoch(model, device, data_loader, optimizer, epoch)\n",
    "    save_checkpoint(model, optimizer, epoch)\n",
    "\n",
    "import horovod.torch as hvd\n",
    "\n",
    "def train_hvd(learning_rate):\n",
    "  hvd.init()  # Иницииализация\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  \n",
    "  if device.type == 'cuda':\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "\n",
    "  train_dataset = datasets.MNIST(\n",
    "    root='data-%d'% hvd.rank(),  # каждый обработчики в своей папке\n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "  )\n",
    "\n",
    "  from torch.utils.data.distributed import DistributedSampler\n",
    "  \n",
    "  train_sampler = DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "  model = Net().to(device)\n",
    "  \n",
    "  optimizer = optim.SGD(model.parameters(), lr=learning_rate * hvd.size(), momentum=momentum)\n",
    "\n",
    "  # оборачиваем оптимизатор в Horovod DistributedOptimizer\n",
    "  optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n",
    "  \n",
    "  # Ставим для всех моделей начальные параметры одинаковыми\n",
    "  hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    train_one_epoch(model, device, train_loader, optimizer, epoch)\n",
    "    # Сохраняем только в одном обработчике\n",
    "    if hvd.rank() == 0:\n",
    "      save_checkpoint(model, optimizer, epoch)\n",
    "  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "  train_hvd(0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srAHAFohPwA1"
   },
   "outputs": [],
   "source": [
    "from learn_hvd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_-HisF2HzVg"
   },
   "source": [
    "### Обучение MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8WfZ2cl590da",
    "outputId": "9cee246c-ec7a-424a-86f2-fc2dce282034"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/learn_hvd.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.361788\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.274706\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.295054\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.246834\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.238976\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.138915\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.160299\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.056116\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 1.922915\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 1.867282\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.603615\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.694041\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.558566\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.332901\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.320291\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.354439\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.175195\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.178067\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.099002\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.088101\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.933984\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.080585\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.886303\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.918420\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.907527\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.853801\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.785874\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.715636\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.829485\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.731614\n"
     ]
    }
   ],
   "source": [
    "train(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYuKlRN4KEu5"
   },
   "source": [
    "### Horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7MHkBDZQe3T",
    "outputId": "cc16ce1e-a15d-4598-bd10-eb903f4649b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: horovod[pytorch] in /usr/local/lib/python3.6/dist-packages (0.20.3)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from horovod[pytorch]) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from horovod[pytorch]) (5.4.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from horovod[pytorch]) (3.13)\n",
      "Requirement already satisfied: cffi>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from horovod[pytorch]) (1.14.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from horovod[pytorch]) (0.7)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from horovod[pytorch]) (1.6.0+cu101)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.4.0->horovod[pytorch]) (2.20)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->horovod[pytorch]) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->horovod[pytorch]) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install horovod[all-frameworks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NdO6SoaSOw6",
    "outputId": "aba50339-23d0-4dfb-91d7-f754adbf0e4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: horovodrun [-h] [-v] -np NP [-cb] [--disable-cache]\n",
      "                  [--start-timeout START_TIMEOUT] [--network-interface NICS]\n",
      "                  [--output-filename OUTPUT_FILENAME] [--verbose]\n",
      "                  [--config-file CONFIG_FILE] [-p SSH_PORT]\n",
      "                  [-i SSH_IDENTITY_FILE]\n",
      "                  [--fusion-threshold-mb FUSION_THRESHOLD_MB]\n",
      "                  [--cycle-time-ms CYCLE_TIME_MS]\n",
      "                  [--cache-capacity CACHE_CAPACITY]\n",
      "                  [--hierarchical-allreduce | --no-hierarchical-allreduce]\n",
      "                  [--hierarchical-allgather | --no-hierarchical-allgather]\n",
      "                  [--autotune] [--autotune-log-file AUTOTUNE_LOG_FILE]\n",
      "                  [--autotune-warmup-samples AUTOTUNE_WARMUP_SAMPLES]\n",
      "                  [--autotune-steps-per-sample AUTOTUNE_STEPS_PER_SAMPLE]\n",
      "                  [--autotune-bayes-opt-max-samples AUTOTUNE_BAYES_OPT_MAX_SAMPLES]\n",
      "                  [--autotune-gaussian-process-noise AUTOTUNE_GAUSSIAN_PROCESS_NOISE]\n",
      "                  [--min-np MIN_NP] [--max-np MAX_NP] [--slots-per-host SLOTS]\n",
      "                  [--elastic-timeout ELASTIC_TIMEOUT]\n",
      "                  [--reset-limit RESET_LIMIT]\n",
      "                  [--timeline-filename TIMELINE_FILENAME]\n",
      "                  [--timeline-mark-cycles] [--no-stall-check]\n",
      "                  [--stall-check-warning-time-seconds STALL_CHECK_WARNING_TIME_SECONDS]\n",
      "                  [--stall-check-shutdown-time-seconds STALL_CHECK_SHUTDOWN_TIME_SECONDS]\n",
      "                  [--mpi-threads-disable] [--mpi-args MPI_ARGS] [--tcp]\n",
      "                  [--binding-args BINDING_ARGS]\n",
      "                  [--num-nccl-streams NUM_NCCL_STREAMS]\n",
      "                  [--ccl-bgt-affinity CCL_BGT_AFFINITY]\n",
      "                  [--gloo-timeout-seconds GLOO_TIMEOUT_SECONDS]\n",
      "                  [--log-level {TRACE,DEBUG,INFO,WARNING,ERROR,FATAL}]\n",
      "                  [--log-hide-timestamp]\n",
      "                  [-H HOSTS | -hostfile HOSTFILE | --host-discovery-script HOST_DISCOVERY_SCRIPT]\n",
      "                  [--gloo | --mpi | --jsrun]\n",
      "                  ...\n",
      "horovodrun: error: the following arguments are required: -np/--num-proc\n"
     ]
    }
   ],
   "source": [
    "! horovodrun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8H8zz3VXTBox",
    "outputId": "773d80ef-5c53-42db-87c6-66c97ff37568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 14:29:12.286241: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "[1,0]<stdout>:Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.337323\n",
      "[1,0]<stdout>:Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.311920\n",
      "[1,0]<stdout>:Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.293706\n",
      "[1,0]<stdout>:Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.302980\n",
      "[1,0]<stdout>:Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.274255\n",
      "[1,0]<stdout>:Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.266550\n",
      "[1,0]<stdout>:Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.245056\n",
      "[1,0]<stdout>:Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.221961\n",
      "[1,0]<stdout>:Train Epoch: 2 [20000/60000 (33%)]\tLoss: 2.153626\n",
      "[1,0]<stdout>:Train Epoch: 2 [30000/60000 (50%)]\tLoss: 2.129339\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! horovodrun -np 1 -H localhost:1 python3 learn_hvd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLG8A8QBTruv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "learn_hvd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:ml-env]",
   "language": "python",
   "name": "conda-env-ml-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
